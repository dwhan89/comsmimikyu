{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from cosmikyu import visualization as covis\n",
    "from cosmikyu import gan, config\n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import mlflow\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = config.default_data_dir\n",
    "mnist_dir = os.path.join(data_dir, 'mnist')\n",
    "cuda = False\n",
    "shape = (1,28,28)\n",
    "latent_dim = 100\n",
    "sample_interval = 1000\n",
    "save_interval = 50000\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure data loader\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(mnist_dir, exist_ok=True)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        mnist_dir,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading saved states\n",
      "failed to load saved states\n",
      "[Epoch 0/1] [Batch 0/938] [D loss: -8.017841] [G loss: 0.038266]\n",
      "saving states\n",
      "[Epoch 0/1] [Batch 5/938] [D loss: -3.938023] [G loss: 0.052839]\n",
      "[Epoch 0/1] [Batch 10/938] [D loss: 5.341599] [G loss: 0.099901]\n",
      "[Epoch 0/1] [Batch 15/938] [D loss: 18.523251] [G loss: 0.235510]\n",
      "[Epoch 0/1] [Batch 20/938] [D loss: 29.054474] [G loss: 0.487936]\n",
      "[Epoch 0/1] [Batch 25/938] [D loss: 32.816338] [G loss: 0.767252]\n",
      "[Epoch 0/1] [Batch 30/938] [D loss: 34.258266] [G loss: 1.010601]\n",
      "[Epoch 0/1] [Batch 35/938] [D loss: 33.607262] [G loss: 1.282418]\n",
      "[Epoch 0/1] [Batch 40/938] [D loss: 33.999588] [G loss: 1.532189]\n",
      "[Epoch 0/1] [Batch 45/938] [D loss: 34.089283] [G loss: 1.851242]\n",
      "[Epoch 0/1] [Batch 50/938] [D loss: 32.675175] [G loss: 2.212576]\n",
      "[Epoch 0/1] [Batch 55/938] [D loss: 33.637203] [G loss: 2.539676]\n",
      "[Epoch 0/1] [Batch 60/938] [D loss: 32.648293] [G loss: 2.939202]\n",
      "[Epoch 0/1] [Batch 65/938] [D loss: 31.925140] [G loss: 3.363510]\n",
      "[Epoch 0/1] [Batch 70/938] [D loss: 31.490519] [G loss: 3.748883]\n",
      "[Epoch 0/1] [Batch 75/938] [D loss: 30.878429] [G loss: 4.155797]\n",
      "[Epoch 0/1] [Batch 80/938] [D loss: 30.427704] [G loss: 4.657819]\n",
      "[Epoch 0/1] [Batch 85/938] [D loss: 30.198290] [G loss: 5.238557]\n",
      "[Epoch 0/1] [Batch 90/938] [D loss: 30.269444] [G loss: 5.705792]\n",
      "[Epoch 0/1] [Batch 95/938] [D loss: 28.836645] [G loss: 6.476366]\n",
      "[Epoch 0/1] [Batch 100/938] [D loss: 27.860527] [G loss: 7.090471]\n",
      "[Epoch 0/1] [Batch 105/938] [D loss: 27.461206] [G loss: 7.852139]\n",
      "[Epoch 0/1] [Batch 110/938] [D loss: 26.619427] [G loss: 8.431624]\n",
      "[Epoch 0/1] [Batch 115/938] [D loss: 25.423176] [G loss: 9.571144]\n",
      "[Epoch 0/1] [Batch 120/938] [D loss: 24.233833] [G loss: 10.826768]\n",
      "[Epoch 0/1] [Batch 125/938] [D loss: 23.248905] [G loss: 11.389622]\n",
      "[Epoch 0/1] [Batch 130/938] [D loss: 21.729580] [G loss: 11.963507]\n",
      "[Epoch 0/1] [Batch 135/938] [D loss: 21.081367] [G loss: 12.577441]\n",
      "[Epoch 0/1] [Batch 140/938] [D loss: 18.654518] [G loss: 14.541142]\n",
      "[Epoch 0/1] [Batch 145/938] [D loss: 17.235113] [G loss: 15.512363]\n",
      "[Epoch 0/1] [Batch 150/938] [D loss: 16.861664] [G loss: 15.660465]\n",
      "[Epoch 0/1] [Batch 155/938] [D loss: 14.422882] [G loss: 17.272121]\n",
      "[Epoch 0/1] [Batch 160/938] [D loss: 14.001924] [G loss: 16.888287]\n",
      "[Epoch 0/1] [Batch 165/938] [D loss: 11.520755] [G loss: 18.918869]\n",
      "[Epoch 0/1] [Batch 170/938] [D loss: 12.106771] [G loss: 17.699379]\n",
      "[Epoch 0/1] [Batch 175/938] [D loss: 11.178005] [G loss: 17.813976]\n",
      "[Epoch 0/1] [Batch 180/938] [D loss: 8.713572] [G loss: 19.488167]\n",
      "[Epoch 0/1] [Batch 185/938] [D loss: 8.303798] [G loss: 19.399231]\n",
      "[Epoch 0/1] [Batch 190/938] [D loss: 7.225672] [G loss: 19.758457]\n",
      "[Epoch 0/1] [Batch 195/938] [D loss: 5.680209] [G loss: 19.649685]\n",
      "[Epoch 0/1] [Batch 200/938] [D loss: 6.113470] [G loss: 18.906170]\n",
      "[Epoch 0/1] [Batch 205/938] [D loss: 5.513421] [G loss: 19.534468]\n",
      "[Epoch 0/1] [Batch 210/938] [D loss: 4.340600] [G loss: 19.596750]\n",
      "[Epoch 0/1] [Batch 215/938] [D loss: 5.087080] [G loss: 18.769499]\n",
      "[Epoch 0/1] [Batch 220/938] [D loss: 4.205401] [G loss: 19.169371]\n",
      "[Epoch 0/1] [Batch 225/938] [D loss: 4.137345] [G loss: 18.412800]\n",
      "[Epoch 0/1] [Batch 230/938] [D loss: 3.092872] [G loss: 19.630848]\n",
      "[Epoch 0/1] [Batch 235/938] [D loss: 3.248036] [G loss: 18.166752]\n",
      "[Epoch 0/1] [Batch 240/938] [D loss: 3.651201] [G loss: 17.713741]\n",
      "[Epoch 0/1] [Batch 245/938] [D loss: 3.591636] [G loss: 17.311853]\n",
      "[Epoch 0/1] [Batch 250/938] [D loss: 3.569474] [G loss: 17.319557]\n",
      "[Epoch 0/1] [Batch 255/938] [D loss: 2.971191] [G loss: 17.762863]\n",
      "[Epoch 0/1] [Batch 260/938] [D loss: 2.825872] [G loss: 17.224525]\n",
      "[Epoch 0/1] [Batch 265/938] [D loss: 2.123131] [G loss: 17.159046]\n",
      "[Epoch 0/1] [Batch 270/938] [D loss: 2.230929] [G loss: 17.232340]\n",
      "[Epoch 0/1] [Batch 275/938] [D loss: 1.941799] [G loss: 17.042370]\n",
      "[Epoch 0/1] [Batch 280/938] [D loss: 1.799419] [G loss: 17.010731]\n",
      "[Epoch 0/1] [Batch 285/938] [D loss: 2.167554] [G loss: 17.332396]\n",
      "[Epoch 0/1] [Batch 290/938] [D loss: 2.293730] [G loss: 16.804792]\n",
      "[Epoch 0/1] [Batch 295/938] [D loss: 1.723576] [G loss: 17.323204]\n",
      "[Epoch 0/1] [Batch 300/938] [D loss: 2.125191] [G loss: 16.504265]\n",
      "[Epoch 0/1] [Batch 305/938] [D loss: 1.889520] [G loss: 17.037655]\n",
      "[Epoch 0/1] [Batch 310/938] [D loss: 1.880135] [G loss: 16.159538]\n",
      "[Epoch 0/1] [Batch 315/938] [D loss: 2.194970] [G loss: 16.069389]\n",
      "[Epoch 0/1] [Batch 320/938] [D loss: 2.017210] [G loss: 15.511863]\n",
      "[Epoch 0/1] [Batch 325/938] [D loss: 1.887833] [G loss: 16.047897]\n",
      "[Epoch 0/1] [Batch 330/938] [D loss: 1.905940] [G loss: 15.703974]\n",
      "[Epoch 0/1] [Batch 335/938] [D loss: 2.426430] [G loss: 16.215057]\n",
      "[Epoch 0/1] [Batch 340/938] [D loss: 1.597004] [G loss: 15.880174]\n",
      "[Epoch 0/1] [Batch 345/938] [D loss: 2.145563] [G loss: 15.294252]\n",
      "[Epoch 0/1] [Batch 350/938] [D loss: 2.221834] [G loss: 14.637224]\n",
      "[Epoch 0/1] [Batch 355/938] [D loss: 1.941927] [G loss: 14.222504]\n",
      "[Epoch 0/1] [Batch 360/938] [D loss: 2.266902] [G loss: 14.381196]\n",
      "[Epoch 0/1] [Batch 365/938] [D loss: 2.156588] [G loss: 13.451294]\n",
      "[Epoch 0/1] [Batch 370/938] [D loss: 2.265252] [G loss: 13.520942]\n",
      "[Epoch 0/1] [Batch 375/938] [D loss: 2.333122] [G loss: 12.421150]\n",
      "[Epoch 0/1] [Batch 380/938] [D loss: 2.433237] [G loss: 13.125631]\n",
      "[Epoch 0/1] [Batch 385/938] [D loss: 1.980045] [G loss: 13.370280]\n",
      "[Epoch 0/1] [Batch 390/938] [D loss: 2.520182] [G loss: 12.896384]\n",
      "[Epoch 0/1] [Batch 395/938] [D loss: 2.436818] [G loss: 12.395047]\n",
      "[Epoch 0/1] [Batch 400/938] [D loss: 2.832892] [G loss: 11.820997]\n",
      "[Epoch 0/1] [Batch 405/938] [D loss: 3.016884] [G loss: 10.092587]\n",
      "[Epoch 0/1] [Batch 410/938] [D loss: 3.165026] [G loss: 9.216553]\n",
      "[Epoch 0/1] [Batch 415/938] [D loss: 2.666878] [G loss: 7.424363]\n",
      "[Epoch 0/1] [Batch 420/938] [D loss: 3.040036] [G loss: 4.381993]\n",
      "[Epoch 0/1] [Batch 425/938] [D loss: 3.299953] [G loss: 5.567943]\n",
      "[Epoch 0/1] [Batch 430/938] [D loss: 3.004893] [G loss: 5.622364]\n",
      "[Epoch 0/1] [Batch 435/938] [D loss: 3.871728] [G loss: 4.924284]\n",
      "[Epoch 0/1] [Batch 440/938] [D loss: 3.298276] [G loss: 6.226727]\n",
      "[Epoch 0/1] [Batch 445/938] [D loss: 3.995743] [G loss: 6.719515]\n",
      "[Epoch 0/1] [Batch 450/938] [D loss: 3.709415] [G loss: 6.260096]\n",
      "[Epoch 0/1] [Batch 455/938] [D loss: 3.713167] [G loss: 4.600712]\n",
      "[Epoch 0/1] [Batch 460/938] [D loss: 3.556742] [G loss: 4.321890]\n",
      "[Epoch 0/1] [Batch 465/938] [D loss: 3.734887] [G loss: 5.361206]\n",
      "[Epoch 0/1] [Batch 470/938] [D loss: 3.397298] [G loss: 6.284877]\n",
      "[Epoch 0/1] [Batch 475/938] [D loss: 4.015406] [G loss: 6.516386]\n",
      "[Epoch 0/1] [Batch 480/938] [D loss: 3.687394] [G loss: 6.657460]\n",
      "[Epoch 0/1] [Batch 485/938] [D loss: 3.663211] [G loss: 5.446221]\n",
      "[Epoch 0/1] [Batch 490/938] [D loss: 3.910244] [G loss: 5.441352]\n",
      "[Epoch 0/1] [Batch 495/938] [D loss: 3.345691] [G loss: 6.962090]\n",
      "[Epoch 0/1] [Batch 500/938] [D loss: 3.788928] [G loss: 6.300653]\n",
      "[Epoch 0/1] [Batch 505/938] [D loss: 3.287893] [G loss: 6.830517]\n",
      "[Epoch 0/1] [Batch 510/938] [D loss: 4.208192] [G loss: 4.885293]\n",
      "[Epoch 0/1] [Batch 515/938] [D loss: 3.911767] [G loss: 4.556220]\n",
      "[Epoch 0/1] [Batch 520/938] [D loss: 3.694695] [G loss: 6.022717]\n",
      "[Epoch 0/1] [Batch 525/938] [D loss: 3.928466] [G loss: 5.986378]\n",
      "[Epoch 0/1] [Batch 530/938] [D loss: 4.062058] [G loss: 5.154916]\n",
      "[Epoch 0/1] [Batch 535/938] [D loss: 4.175531] [G loss: 5.776219]\n",
      "[Epoch 0/1] [Batch 540/938] [D loss: 4.301030] [G loss: 5.604463]\n",
      "[Epoch 0/1] [Batch 545/938] [D loss: 3.914009] [G loss: 3.594283]\n",
      "[Epoch 0/1] [Batch 550/938] [D loss: 4.465366] [G loss: 5.189627]\n",
      "[Epoch 0/1] [Batch 555/938] [D loss: 4.393176] [G loss: 5.091165]\n",
      "[Epoch 0/1] [Batch 560/938] [D loss: 4.178119] [G loss: 5.483950]\n",
      "[Epoch 0/1] [Batch 565/938] [D loss: 3.224179] [G loss: 5.672978]\n",
      "[Epoch 0/1] [Batch 570/938] [D loss: 4.233075] [G loss: 5.811004]\n",
      "[Epoch 0/1] [Batch 575/938] [D loss: 4.038364] [G loss: 5.204300]\n",
      "[Epoch 0/1] [Batch 580/938] [D loss: 4.429492] [G loss: 5.570952]\n",
      "[Epoch 0/1] [Batch 585/938] [D loss: 3.642068] [G loss: 5.536787]\n",
      "[Epoch 0/1] [Batch 590/938] [D loss: 3.911464] [G loss: 6.140562]\n",
      "[Epoch 0/1] [Batch 595/938] [D loss: 4.517015] [G loss: 4.826337]\n",
      "[Epoch 0/1] [Batch 600/938] [D loss: 4.094501] [G loss: 5.453237]\n",
      "[Epoch 0/1] [Batch 605/938] [D loss: 4.447817] [G loss: 5.530167]\n",
      "[Epoch 0/1] [Batch 610/938] [D loss: 4.386790] [G loss: 5.702699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/1] [Batch 615/938] [D loss: 4.429830] [G loss: 5.395204]\n",
      "[Epoch 0/1] [Batch 620/938] [D loss: 4.344399] [G loss: 4.624345]\n",
      "[Epoch 0/1] [Batch 625/938] [D loss: 4.474028] [G loss: 3.741048]\n",
      "[Epoch 0/1] [Batch 630/938] [D loss: 4.463370] [G loss: 4.557129]\n",
      "[Epoch 0/1] [Batch 635/938] [D loss: 4.331319] [G loss: 4.280912]\n",
      "[Epoch 0/1] [Batch 640/938] [D loss: 4.225828] [G loss: 4.916999]\n",
      "[Epoch 0/1] [Batch 645/938] [D loss: 4.767808] [G loss: 4.052751]\n",
      "[Epoch 0/1] [Batch 650/938] [D loss: 4.462874] [G loss: 5.046508]\n",
      "[Epoch 0/1] [Batch 655/938] [D loss: 4.748121] [G loss: 5.697442]\n",
      "[Epoch 0/1] [Batch 660/938] [D loss: 5.078349] [G loss: 3.043800]\n",
      "[Epoch 0/1] [Batch 665/938] [D loss: 4.550309] [G loss: 4.878983]\n",
      "[Epoch 0/1] [Batch 670/938] [D loss: 4.955491] [G loss: 3.595835]\n",
      "[Epoch 0/1] [Batch 675/938] [D loss: 4.905604] [G loss: 3.438066]\n",
      "[Epoch 0/1] [Batch 680/938] [D loss: 4.262588] [G loss: 4.721101]\n",
      "[Epoch 0/1] [Batch 685/938] [D loss: 5.101101] [G loss: 3.003175]\n",
      "[Epoch 0/1] [Batch 690/938] [D loss: 4.830819] [G loss: 3.456344]\n",
      "[Epoch 0/1] [Batch 695/938] [D loss: 4.774027] [G loss: 4.089826]\n",
      "[Epoch 0/1] [Batch 700/938] [D loss: 4.555701] [G loss: 4.071389]\n",
      "[Epoch 0/1] [Batch 705/938] [D loss: 4.620844] [G loss: 3.379609]\n",
      "[Epoch 0/1] [Batch 710/938] [D loss: 4.891349] [G loss: 4.294836]\n",
      "[Epoch 0/1] [Batch 715/938] [D loss: 4.541059] [G loss: 5.294101]\n",
      "[Epoch 0/1] [Batch 720/938] [D loss: 5.016391] [G loss: 4.056186]\n",
      "[Epoch 0/1] [Batch 725/938] [D loss: 4.409185] [G loss: 4.849282]\n",
      "[Epoch 0/1] [Batch 730/938] [D loss: 4.587230] [G loss: 3.833598]\n",
      "[Epoch 0/1] [Batch 735/938] [D loss: 4.896955] [G loss: 3.358443]\n",
      "[Epoch 0/1] [Batch 740/938] [D loss: 4.656451] [G loss: 4.465973]\n",
      "[Epoch 0/1] [Batch 745/938] [D loss: 4.513803] [G loss: 4.573393]\n",
      "[Epoch 0/1] [Batch 750/938] [D loss: 4.672173] [G loss: 2.704278]\n",
      "[Epoch 0/1] [Batch 755/938] [D loss: 4.351280] [G loss: 4.420888]\n",
      "[Epoch 0/1] [Batch 760/938] [D loss: 4.827829] [G loss: 3.702612]\n",
      "[Epoch 0/1] [Batch 765/938] [D loss: 4.399265] [G loss: 5.164680]\n",
      "[Epoch 0/1] [Batch 770/938] [D loss: 4.282094] [G loss: 4.535737]\n",
      "[Epoch 0/1] [Batch 775/938] [D loss: 4.607858] [G loss: 5.055406]\n",
      "[Epoch 0/1] [Batch 780/938] [D loss: 4.265254] [G loss: 4.472997]\n",
      "[Epoch 0/1] [Batch 785/938] [D loss: 4.897511] [G loss: 3.885504]\n",
      "[Epoch 0/1] [Batch 790/938] [D loss: 4.168096] [G loss: 3.993656]\n",
      "[Epoch 0/1] [Batch 795/938] [D loss: 4.590474] [G loss: 3.798044]\n",
      "[Epoch 0/1] [Batch 800/938] [D loss: 4.139572] [G loss: 4.023503]\n",
      "[Epoch 0/1] [Batch 805/938] [D loss: 3.968114] [G loss: 4.484902]\n",
      "[Epoch 0/1] [Batch 810/938] [D loss: 4.479801] [G loss: 4.111997]\n",
      "[Epoch 0/1] [Batch 815/938] [D loss: 4.217099] [G loss: 3.532969]\n",
      "[Epoch 0/1] [Batch 820/938] [D loss: 3.870997] [G loss: 2.740799]\n",
      "[Epoch 0/1] [Batch 825/938] [D loss: 4.532566] [G loss: 4.000281]\n",
      "[Epoch 0/1] [Batch 830/938] [D loss: 4.755365] [G loss: 3.689148]\n",
      "[Epoch 0/1] [Batch 835/938] [D loss: 3.978700] [G loss: 4.337735]\n",
      "[Epoch 0/1] [Batch 840/938] [D loss: 4.228334] [G loss: 3.687392]\n",
      "[Epoch 0/1] [Batch 845/938] [D loss: 4.198194] [G loss: 4.246320]\n",
      "[Epoch 0/1] [Batch 850/938] [D loss: 4.706959] [G loss: 3.902032]\n",
      "[Epoch 0/1] [Batch 855/938] [D loss: 4.520467] [G loss: 3.441922]\n",
      "[Epoch 0/1] [Batch 860/938] [D loss: 4.437611] [G loss: 3.215433]\n",
      "[Epoch 0/1] [Batch 865/938] [D loss: 4.474975] [G loss: 2.580730]\n",
      "[Epoch 0/1] [Batch 870/938] [D loss: 4.620235] [G loss: 4.402109]\n",
      "[Epoch 0/1] [Batch 875/938] [D loss: 4.493412] [G loss: 2.765428]\n",
      "[Epoch 0/1] [Batch 880/938] [D loss: 4.930887] [G loss: 2.530620]\n",
      "[Epoch 0/1] [Batch 885/938] [D loss: 4.444426] [G loss: 3.316699]\n",
      "[Epoch 0/1] [Batch 890/938] [D loss: 4.406315] [G loss: 2.310117]\n",
      "[Epoch 0/1] [Batch 895/938] [D loss: 4.682045] [G loss: 3.562174]\n",
      "[Epoch 0/1] [Batch 900/938] [D loss: 4.594917] [G loss: 4.457751]\n",
      "[Epoch 0/1] [Batch 905/938] [D loss: 5.021867] [G loss: 2.353180]\n",
      "[Epoch 0/1] [Batch 910/938] [D loss: 4.145600] [G loss: 3.784801]\n",
      "[Epoch 0/1] [Batch 915/938] [D loss: 5.249261] [G loss: 2.907424]\n",
      "[Epoch 0/1] [Batch 920/938] [D loss: 4.720278] [G loss: 2.712710]\n",
      "[Epoch 0/1] [Batch 925/938] [D loss: 4.770544] [G loss: 3.212096]\n",
      "[Epoch 0/1] [Batch 930/938] [D loss: 4.692363] [G loss: 3.631293]\n",
      "[Epoch 0/1] [Batch 935/938] [D loss: 4.824657] [G loss: 4.077546]\n",
      "saving states\n"
     ]
    }
   ],
   "source": [
    "WGAN_GP = gan.WGAN_GP(\"mnist_wgan_gp\", shape, latent_dim, cuda=True, ngpu=4)\n",
    "mlflow.set_experiment(WGAN_GP.identifier)\n",
    "with mlflow.start_run(experiment_id=WGAN_GP.experiment.experiment_id) as mlflow_run:\n",
    "    WGAN_GP.train(\n",
    "        dataloader,\n",
    "        nepochs=200,\n",
    "        ncritics=5,\n",
    "        sample_interval=1000,\n",
    "        save_interval=10000,\n",
    "        load_states=True,\n",
    "        save_states=True,\n",
    "        verbose=True,\n",
    "        mlflow_run=mlflow_run,\n",
    "        lr=2e-04,\n",
    "        betas=(0.5, 0.999),\n",
    "        lambda_gp=10,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WGAN = gan.WGAN(\"mnist_wgan\", shape, latent_dim, cuda=True, ngpu=4)\n",
    "mlflow.set_experiment(WGAN.identifier)\n",
    "with mlflow.start_run(experiment_id=WGAN.experiment.experiment_id) as mlflow_run:\n",
    "    WGAN.train(\n",
    "        dataloader,\n",
    "        nepochs=1,\n",
    "        ncritics=5,\n",
    "        sample_interval=1000,\n",
    "        save_interval=10000,\n",
    "        load_states=True,\n",
    "        save_states=True,\n",
    "        verbose=True,\n",
    "        mlflow_run=mlflow_run,\n",
    "        lr=5e-05,\n",
    "        clip_tresh=0.01,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading saved states\n",
      "failed to load saved states\n",
      "[Epoch 0/1] [Batch 0/938] [D loss: 0.149059] [G loss: -0.009395]\n",
      "saving states\n",
      "[Epoch 0/1] [Batch 5/938] [D loss: 0.115641] [G loss: -0.008499]\n",
      "[Epoch 0/1] [Batch 10/938] [D loss: 0.367596] [G loss: -0.005231]\n",
      "[Epoch 0/1] [Batch 15/938] [D loss: 0.805645] [G loss: 0.006488]\n",
      "[Epoch 0/1] [Batch 20/938] [D loss: 1.372553] [G loss: 0.032683]\n",
      "[Epoch 0/1] [Batch 25/938] [D loss: 1.984034] [G loss: 0.072270]\n",
      "[Epoch 0/1] [Batch 30/938] [D loss: 2.680871] [G loss: 0.136101]\n",
      "[Epoch 0/1] [Batch 35/938] [D loss: 3.365093] [G loss: 0.216362]\n",
      "[Epoch 0/1] [Batch 40/938] [D loss: 4.089509] [G loss: 0.321988]\n",
      "[Epoch 0/1] [Batch 45/938] [D loss: 4.789481] [G loss: 0.429416]\n",
      "[Epoch 0/1] [Batch 50/938] [D loss: 5.334355] [G loss: 0.576705]\n",
      "[Epoch 0/1] [Batch 55/938] [D loss: 6.014461] [G loss: 0.745393]\n",
      "[Epoch 0/1] [Batch 60/938] [D loss: 6.722319] [G loss: 0.946569]\n",
      "[Epoch 0/1] [Batch 65/938] [D loss: 7.354253] [G loss: 1.161500]\n",
      "[Epoch 0/1] [Batch 70/938] [D loss: 7.939179] [G loss: 1.414079]\n",
      "[Epoch 0/1] [Batch 75/938] [D loss: 8.642735] [G loss: 1.667922]\n",
      "[Epoch 0/1] [Batch 80/938] [D loss: 8.686020] [G loss: 2.000668]\n",
      "[Epoch 0/1] [Batch 85/938] [D loss: 9.264610] [G loss: 2.327451]\n",
      "[Epoch 0/1] [Batch 90/938] [D loss: 9.607288] [G loss: 2.714776]\n",
      "[Epoch 0/1] [Batch 95/938] [D loss: 9.871639] [G loss: 3.083445]\n",
      "[Epoch 0/1] [Batch 100/938] [D loss: 10.285230] [G loss: 3.583124]\n",
      "[Epoch 0/1] [Batch 105/938] [D loss: 11.120618] [G loss: 3.775237]\n",
      "[Epoch 0/1] [Batch 110/938] [D loss: 10.952147] [G loss: 4.527491]\n",
      "[Epoch 0/1] [Batch 115/938] [D loss: 10.554180] [G loss: 5.193058]\n",
      "[Epoch 0/1] [Batch 120/938] [D loss: 11.422714] [G loss: 5.212765]\n",
      "[Epoch 0/1] [Batch 125/938] [D loss: 12.092981] [G loss: 5.726617]\n",
      "[Epoch 0/1] [Batch 130/938] [D loss: 11.372392] [G loss: 6.444405]\n",
      "[Epoch 0/1] [Batch 135/938] [D loss: 11.743587] [G loss: 6.917424]\n",
      "[Epoch 0/1] [Batch 140/938] [D loss: 11.591875] [G loss: 8.015040]\n",
      "[Epoch 0/1] [Batch 145/938] [D loss: 11.641474] [G loss: 8.404589]\n",
      "[Epoch 0/1] [Batch 150/938] [D loss: 11.584969] [G loss: 8.938066]\n",
      "[Epoch 0/1] [Batch 155/938] [D loss: 11.925917] [G loss: 9.423714]\n",
      "[Epoch 0/1] [Batch 160/938] [D loss: 11.456375] [G loss: 10.422586]\n",
      "[Epoch 0/1] [Batch 165/938] [D loss: 10.915915] [G loss: 11.290276]\n",
      "[Epoch 0/1] [Batch 170/938] [D loss: 10.630375] [G loss: 11.962985]\n",
      "[Epoch 0/1] [Batch 175/938] [D loss: 10.815525] [G loss: 11.970399]\n",
      "[Epoch 0/1] [Batch 180/938] [D loss: 10.680614] [G loss: 12.745106]\n",
      "[Epoch 0/1] [Batch 185/938] [D loss: 10.198082] [G loss: 14.249184]\n",
      "[Epoch 0/1] [Batch 190/938] [D loss: 8.842313] [G loss: 15.454256]\n",
      "[Epoch 0/1] [Batch 195/938] [D loss: 9.236538] [G loss: 15.036652]\n",
      "[Epoch 0/1] [Batch 200/938] [D loss: 8.912037] [G loss: 16.039125]\n",
      "[Epoch 0/1] [Batch 205/938] [D loss: 9.976908] [G loss: 15.534472]\n",
      "[Epoch 0/1] [Batch 210/938] [D loss: 8.894073] [G loss: 16.823347]\n",
      "[Epoch 0/1] [Batch 215/938] [D loss: 8.032223] [G loss: 18.076723]\n",
      "[Epoch 0/1] [Batch 220/938] [D loss: 8.262661] [G loss: 17.698193]\n",
      "[Epoch 0/1] [Batch 225/938] [D loss: 7.522724] [G loss: 18.658136]\n",
      "[Epoch 0/1] [Batch 230/938] [D loss: 7.466599] [G loss: 19.073980]\n",
      "[Epoch 0/1] [Batch 235/938] [D loss: 6.874619] [G loss: 19.584568]\n",
      "[Epoch 0/1] [Batch 240/938] [D loss: 7.614761] [G loss: 19.246429]\n",
      "[Epoch 0/1] [Batch 245/938] [D loss: 7.098154] [G loss: 19.586811]\n",
      "[Epoch 0/1] [Batch 250/938] [D loss: 6.411819] [G loss: 20.403458]\n",
      "[Epoch 0/1] [Batch 255/938] [D loss: 6.203115] [G loss: 20.620983]\n",
      "[Epoch 0/1] [Batch 260/938] [D loss: 6.255688] [G loss: 21.167110]\n",
      "[Epoch 0/1] [Batch 265/938] [D loss: 6.155010] [G loss: 21.071430]\n",
      "[Epoch 0/1] [Batch 270/938] [D loss: 5.561272] [G loss: 21.544130]\n",
      "[Epoch 0/1] [Batch 275/938] [D loss: 5.519657] [G loss: 21.538628]\n",
      "[Epoch 0/1] [Batch 280/938] [D loss: 5.695107] [G loss: 21.213810]\n",
      "[Epoch 0/1] [Batch 285/938] [D loss: 4.784199] [G loss: 22.398825]\n",
      "[Epoch 0/1] [Batch 290/938] [D loss: 5.626009] [G loss: 21.544975]\n",
      "[Epoch 0/1] [Batch 295/938] [D loss: 5.167927] [G loss: 21.897932]\n",
      "[Epoch 0/1] [Batch 300/938] [D loss: 5.629650] [G loss: 21.335218]\n",
      "[Epoch 0/1] [Batch 305/938] [D loss: 5.074512] [G loss: 22.048384]\n",
      "[Epoch 0/1] [Batch 310/938] [D loss: 4.529831] [G loss: 22.310667]\n",
      "[Epoch 0/1] [Batch 315/938] [D loss: 5.964012] [G loss: 21.147762]\n",
      "[Epoch 0/1] [Batch 320/938] [D loss: 4.361763] [G loss: 22.648706]\n",
      "[Epoch 0/1] [Batch 325/938] [D loss: 4.657511] [G loss: 22.219759]\n",
      "[Epoch 0/1] [Batch 330/938] [D loss: 3.579426] [G loss: 22.885799]\n",
      "[Epoch 0/1] [Batch 335/938] [D loss: 5.076267] [G loss: 21.479269]\n",
      "[Epoch 0/1] [Batch 340/938] [D loss: 4.413828] [G loss: 21.944000]\n",
      "[Epoch 0/1] [Batch 345/938] [D loss: 4.978354] [G loss: 21.660578]\n",
      "[Epoch 0/1] [Batch 350/938] [D loss: 4.443380] [G loss: 22.127787]\n",
      "[Epoch 0/1] [Batch 355/938] [D loss: 3.746010] [G loss: 22.441811]\n",
      "[Epoch 0/1] [Batch 360/938] [D loss: 3.260822] [G loss: 22.508270]\n",
      "[Epoch 0/1] [Batch 365/938] [D loss: 3.791962] [G loss: 22.361515]\n",
      "[Epoch 0/1] [Batch 370/938] [D loss: 4.358196] [G loss: 21.554329]\n",
      "[Epoch 0/1] [Batch 375/938] [D loss: 3.140953] [G loss: 22.509968]\n",
      "[Epoch 0/1] [Batch 380/938] [D loss: 3.864620] [G loss: 21.929276]\n",
      "[Epoch 0/1] [Batch 385/938] [D loss: 3.452997] [G loss: 21.885071]\n",
      "[Epoch 0/1] [Batch 390/938] [D loss: 4.098167] [G loss: 21.355259]\n",
      "[Epoch 0/1] [Batch 395/938] [D loss: 4.421326] [G loss: 20.954725]\n",
      "[Epoch 0/1] [Batch 400/938] [D loss: 3.599609] [G loss: 21.658762]\n",
      "[Epoch 0/1] [Batch 405/938] [D loss: 3.520409] [G loss: 21.618843]\n",
      "[Epoch 0/1] [Batch 410/938] [D loss: 3.456192] [G loss: 21.675104]\n",
      "[Epoch 0/1] [Batch 415/938] [D loss: 3.600107] [G loss: 21.630989]\n",
      "[Epoch 0/1] [Batch 420/938] [D loss: 3.252056] [G loss: 21.492821]\n",
      "[Epoch 0/1] [Batch 425/938] [D loss: 3.004765] [G loss: 21.535351]\n",
      "[Epoch 0/1] [Batch 430/938] [D loss: 2.977892] [G loss: 21.623955]\n",
      "[Epoch 0/1] [Batch 435/938] [D loss: 3.083122] [G loss: 21.643223]\n",
      "[Epoch 0/1] [Batch 440/938] [D loss: 3.466297] [G loss: 21.010653]\n",
      "[Epoch 0/1] [Batch 445/938] [D loss: 3.500854] [G loss: 20.885593]\n",
      "[Epoch 0/1] [Batch 450/938] [D loss: 2.916626] [G loss: 21.140221]\n",
      "[Epoch 0/1] [Batch 455/938] [D loss: 2.325695] [G loss: 21.555410]\n",
      "[Epoch 0/1] [Batch 460/938] [D loss: 2.924412] [G loss: 20.665798]\n",
      "[Epoch 0/1] [Batch 465/938] [D loss: 2.122858] [G loss: 21.590195]\n",
      "[Epoch 0/1] [Batch 470/938] [D loss: 2.724928] [G loss: 21.050095]\n",
      "[Epoch 0/1] [Batch 475/938] [D loss: 3.020138] [G loss: 20.627617]\n",
      "[Epoch 0/1] [Batch 480/938] [D loss: 2.395147] [G loss: 21.048203]\n",
      "[Epoch 0/1] [Batch 485/938] [D loss: 2.188503] [G loss: 21.142963]\n",
      "[Epoch 0/1] [Batch 490/938] [D loss: 2.166759] [G loss: 20.861118]\n",
      "[Epoch 0/1] [Batch 495/938] [D loss: 2.611816] [G loss: 20.608992]\n",
      "[Epoch 0/1] [Batch 500/938] [D loss: 2.494413] [G loss: 20.683559]\n",
      "[Epoch 0/1] [Batch 505/938] [D loss: 2.916025] [G loss: 20.187614]\n",
      "[Epoch 0/1] [Batch 510/938] [D loss: 2.135441] [G loss: 20.590660]\n",
      "[Epoch 0/1] [Batch 515/938] [D loss: 2.010920] [G loss: 20.895470]\n",
      "[Epoch 0/1] [Batch 520/938] [D loss: 1.881496] [G loss: 20.561810]\n",
      "[Epoch 0/1] [Batch 525/938] [D loss: 2.549068] [G loss: 20.379303]\n",
      "[Epoch 0/1] [Batch 530/938] [D loss: 2.648994] [G loss: 20.082676]\n",
      "[Epoch 0/1] [Batch 535/938] [D loss: 1.909033] [G loss: 20.474546]\n",
      "[Epoch 0/1] [Batch 540/938] [D loss: 2.658205] [G loss: 19.730164]\n",
      "[Epoch 0/1] [Batch 545/938] [D loss: 1.529228] [G loss: 20.117170]\n",
      "[Epoch 0/1] [Batch 550/938] [D loss: 2.086878] [G loss: 20.088959]\n",
      "[Epoch 0/1] [Batch 555/938] [D loss: 2.123844] [G loss: 19.921665]\n",
      "[Epoch 0/1] [Batch 560/938] [D loss: 1.907883] [G loss: 19.931107]\n",
      "[Epoch 0/1] [Batch 565/938] [D loss: 1.968870] [G loss: 19.884377]\n",
      "[Epoch 0/1] [Batch 570/938] [D loss: 1.599049] [G loss: 19.915766]\n",
      "[Epoch 0/1] [Batch 575/938] [D loss: 1.671671] [G loss: 19.769001]\n",
      "[Epoch 0/1] [Batch 580/938] [D loss: 1.699286] [G loss: 19.753359]\n",
      "[Epoch 0/1] [Batch 585/938] [D loss: 2.159317] [G loss: 19.617130]\n",
      "[Epoch 0/1] [Batch 590/938] [D loss: 1.786572] [G loss: 19.356291]\n",
      "[Epoch 0/1] [Batch 595/938] [D loss: 1.948347] [G loss: 19.654842]\n",
      "[Epoch 0/1] [Batch 600/938] [D loss: 1.728588] [G loss: 19.322435]\n",
      "[Epoch 0/1] [Batch 605/938] [D loss: 1.989578] [G loss: 19.348045]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/1] [Batch 610/938] [D loss: 1.620930] [G loss: 19.343304]\n",
      "[Epoch 0/1] [Batch 615/938] [D loss: 2.029818] [G loss: 19.050636]\n",
      "[Epoch 0/1] [Batch 620/938] [D loss: 1.368628] [G loss: 19.084351]\n",
      "[Epoch 0/1] [Batch 625/938] [D loss: 1.486616] [G loss: 19.091068]\n",
      "[Epoch 0/1] [Batch 630/938] [D loss: 1.521614] [G loss: 19.150620]\n",
      "[Epoch 0/1] [Batch 635/938] [D loss: 1.952475] [G loss: 18.691364]\n",
      "[Epoch 0/1] [Batch 640/938] [D loss: 1.375393] [G loss: 19.063080]\n",
      "[Epoch 0/1] [Batch 645/938] [D loss: 1.558311] [G loss: 18.623796]\n",
      "[Epoch 0/1] [Batch 650/938] [D loss: 1.367371] [G loss: 18.871212]\n",
      "[Epoch 0/1] [Batch 655/938] [D loss: 1.621298] [G loss: 18.662251]\n",
      "[Epoch 0/1] [Batch 660/938] [D loss: 1.390577] [G loss: 18.606510]\n",
      "[Epoch 0/1] [Batch 665/938] [D loss: 1.539234] [G loss: 18.722187]\n",
      "[Epoch 0/1] [Batch 670/938] [D loss: 1.320089] [G loss: 18.728756]\n",
      "[Epoch 0/1] [Batch 675/938] [D loss: 1.086983] [G loss: 18.650928]\n",
      "[Epoch 0/1] [Batch 680/938] [D loss: 1.483360] [G loss: 18.551491]\n",
      "[Epoch 0/1] [Batch 685/938] [D loss: 1.409252] [G loss: 18.480984]\n",
      "[Epoch 0/1] [Batch 690/938] [D loss: 0.882681] [G loss: 18.565916]\n",
      "[Epoch 0/1] [Batch 695/938] [D loss: 1.335361] [G loss: 18.356728]\n",
      "[Epoch 0/1] [Batch 700/938] [D loss: 1.685469] [G loss: 18.280931]\n",
      "[Epoch 0/1] [Batch 705/938] [D loss: 1.335131] [G loss: 18.350243]\n",
      "[Epoch 0/1] [Batch 710/938] [D loss: 1.092201] [G loss: 18.342432]\n",
      "[Epoch 0/1] [Batch 715/938] [D loss: 1.047060] [G loss: 18.091045]\n",
      "[Epoch 0/1] [Batch 720/938] [D loss: 1.003017] [G loss: 18.076675]\n",
      "[Epoch 0/1] [Batch 725/938] [D loss: 1.501795] [G loss: 17.884159]\n",
      "[Epoch 0/1] [Batch 730/938] [D loss: 1.190205] [G loss: 17.917080]\n",
      "[Epoch 0/1] [Batch 735/938] [D loss: 1.184626] [G loss: 17.838301]\n",
      "[Epoch 0/1] [Batch 740/938] [D loss: 1.401405] [G loss: 17.761024]\n",
      "[Epoch 0/1] [Batch 745/938] [D loss: 1.321323] [G loss: 17.742290]\n",
      "[Epoch 0/1] [Batch 750/938] [D loss: 1.366179] [G loss: 17.866529]\n",
      "[Epoch 0/1] [Batch 755/938] [D loss: 1.288858] [G loss: 17.742283]\n",
      "[Epoch 0/1] [Batch 760/938] [D loss: 1.109795] [G loss: 17.747799]\n",
      "[Epoch 0/1] [Batch 765/938] [D loss: 1.466251] [G loss: 17.632126]\n",
      "[Epoch 0/1] [Batch 770/938] [D loss: 1.439331] [G loss: 17.502714]\n",
      "[Epoch 0/1] [Batch 775/938] [D loss: 1.158207] [G loss: 17.419655]\n",
      "[Epoch 0/1] [Batch 780/938] [D loss: 0.795124] [G loss: 17.477724]\n",
      "[Epoch 0/1] [Batch 785/938] [D loss: 1.164011] [G loss: 17.417624]\n",
      "[Epoch 0/1] [Batch 790/938] [D loss: 0.913050] [G loss: 17.330189]\n",
      "[Epoch 0/1] [Batch 795/938] [D loss: 0.983866] [G loss: 17.284805]\n",
      "[Epoch 0/1] [Batch 800/938] [D loss: 0.999565] [G loss: 17.218353]\n",
      "[Epoch 0/1] [Batch 805/938] [D loss: 1.392700] [G loss: 17.257919]\n",
      "[Epoch 0/1] [Batch 810/938] [D loss: 1.048775] [G loss: 17.183159]\n",
      "[Epoch 0/1] [Batch 815/938] [D loss: 0.791500] [G loss: 17.130627]\n",
      "[Epoch 0/1] [Batch 820/938] [D loss: 1.180225] [G loss: 17.024342]\n",
      "[Epoch 0/1] [Batch 825/938] [D loss: 1.088238] [G loss: 16.982515]\n",
      "[Epoch 0/1] [Batch 830/938] [D loss: 1.199831] [G loss: 16.952307]\n",
      "[Epoch 0/1] [Batch 835/938] [D loss: 1.440973] [G loss: 16.902719]\n",
      "[Epoch 0/1] [Batch 840/938] [D loss: 0.914240] [G loss: 16.961567]\n",
      "[Epoch 0/1] [Batch 845/938] [D loss: 0.871271] [G loss: 16.913404]\n",
      "[Epoch 0/1] [Batch 850/938] [D loss: 1.127090] [G loss: 16.717875]\n",
      "[Epoch 0/1] [Batch 855/938] [D loss: 1.237921] [G loss: 16.768545]\n",
      "[Epoch 0/1] [Batch 860/938] [D loss: 1.094795] [G loss: 16.787338]\n",
      "[Epoch 0/1] [Batch 865/938] [D loss: 1.137426] [G loss: 16.714121]\n",
      "[Epoch 0/1] [Batch 870/938] [D loss: 1.014626] [G loss: 16.781540]\n",
      "[Epoch 0/1] [Batch 875/938] [D loss: 0.857046] [G loss: 16.571381]\n",
      "[Epoch 0/1] [Batch 880/938] [D loss: 1.546215] [G loss: 16.572613]\n",
      "[Epoch 0/1] [Batch 885/938] [D loss: 1.213337] [G loss: 16.512545]\n",
      "[Epoch 0/1] [Batch 890/938] [D loss: 0.965145] [G loss: 16.517698]\n",
      "[Epoch 0/1] [Batch 895/938] [D loss: 0.901539] [G loss: 16.546251]\n",
      "[Epoch 0/1] [Batch 900/938] [D loss: 1.126135] [G loss: 16.386772]\n",
      "[Epoch 0/1] [Batch 905/938] [D loss: 1.004110] [G loss: 16.489559]\n",
      "[Epoch 0/1] [Batch 910/938] [D loss: 1.280439] [G loss: 16.284855]\n",
      "[Epoch 0/1] [Batch 915/938] [D loss: 0.931026] [G loss: 16.351139]\n",
      "[Epoch 0/1] [Batch 920/938] [D loss: 1.001350] [G loss: 16.395710]\n",
      "[Epoch 0/1] [Batch 925/938] [D loss: 0.873703] [G loss: 16.381556]\n",
      "[Epoch 0/1] [Batch 930/938] [D loss: 0.896889] [G loss: 16.378559]\n",
      "[Epoch 0/1] [Batch 935/938] [D loss: 0.973478] [G loss: 16.250500]\n",
      "saving states\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
