{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cosmikyu import gan, config, model\n",
    "from cosmikyu import nn as cnn\n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import mlflow\n",
    "import torchsummary\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwhan89/.miniconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "WARNING:root:Malformed experiment 'mlruns'. Detailed error Yaml file '/home/dwhan89/workspace/cosmikyu/cosmikyu/../output/mlruns/mlruns/meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dwhan89/.miniconda3/lib/python3.7/site-packages/mlflow/store/tracking/file_store.py\", line 197, in list_experiments\n",
      "    experiment = self._get_experiment(exp_id, view_type)\n",
      "  File \"/home/dwhan89/.miniconda3/lib/python3.7/site-packages/mlflow/store/tracking/file_store.py\", line 260, in _get_experiment\n",
      "    meta = read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/dwhan89/.miniconda3/lib/python3.7/site-packages/mlflow/utils/file_utils.py\", line 167, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/dwhan89/workspace/cosmikyu/cosmikyu/../output/mlruns/mlruns/meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] You have a CUDA device. You probably want to run with CUDA enabled\n",
      "====pixgan======\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [1, 64, 64, 64]           5,184\n",
      "         LeakyReLU-2            [1, 64, 64, 64]               0\n",
      "            Conv2d-3           [1, 128, 32, 32]         131,200\n",
      "         LeakyReLU-4           [1, 128, 32, 32]               0\n",
      "            Conv2d-5           [1, 256, 16, 16]         524,544\n",
      "         LeakyReLU-6           [1, 256, 16, 16]               0\n",
      "            Conv2d-7             [1, 512, 8, 8]       2,097,664\n",
      "         LeakyReLU-8             [1, 512, 8, 8]               0\n",
      "            Conv2d-9            [1, 1024, 4, 4]       8,389,632\n",
      "        LeakyReLU-10            [1, 1024, 4, 4]               0\n",
      "          Reshape-11                 [1, 16384]               0\n",
      "           Linear-12                     [1, 1]          16,385\n",
      "================================================================\n",
      "Total params: 11,164,609\n",
      "Trainable params: 11,164,609\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.31\n",
      "Forward/backward pass size (MB): 7.88\n",
      "Params size (MB): 42.59\n",
      "Estimated Total Size (MB): 50.78\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [1, 64, 64, 64]           1,088\n",
      "         LeakyReLU-2            [1, 64, 64, 64]               0\n",
      "          UNetDown-3            [1, 64, 64, 64]               0\n",
      "            Conv2d-4           [1, 128, 32, 32]         131,200\n",
      "       BatchNorm2d-5           [1, 128, 32, 32]             256\n",
      "         LeakyReLU-6           [1, 128, 32, 32]               0\n",
      "          UNetDown-7           [1, 128, 32, 32]               0\n",
      "            Conv2d-8           [1, 256, 16, 16]         524,544\n",
      "       BatchNorm2d-9           [1, 256, 16, 16]             512\n",
      "        LeakyReLU-10           [1, 256, 16, 16]               0\n",
      "         UNetDown-11           [1, 256, 16, 16]               0\n",
      "           Conv2d-12             [1, 512, 8, 8]       2,097,664\n",
      "      BatchNorm2d-13             [1, 512, 8, 8]           1,024\n",
      "        LeakyReLU-14             [1, 512, 8, 8]               0\n",
      "         UNetDown-15             [1, 512, 8, 8]               0\n",
      "           Conv2d-16             [1, 512, 4, 4]       4,194,816\n",
      "      BatchNorm2d-17             [1, 512, 4, 4]           1,024\n",
      "        LeakyReLU-18             [1, 512, 4, 4]               0\n",
      "         UNetDown-19             [1, 512, 4, 4]               0\n",
      "           Conv2d-20             [1, 512, 2, 2]       4,194,816\n",
      "      BatchNorm2d-21             [1, 512, 2, 2]           1,024\n",
      "        LeakyReLU-22             [1, 512, 2, 2]               0\n",
      "         UNetDown-23             [1, 512, 2, 2]               0\n",
      "           Conv2d-24             [1, 512, 1, 1]       4,194,816\n",
      "             ReLU-25             [1, 512, 1, 1]               0\n",
      "         UNetDown-26             [1, 512, 1, 1]               0\n",
      "  ConvTranspose2d-27             [1, 512, 2, 2]       4,194,816\n",
      "      BatchNorm2d-28             [1, 512, 2, 2]           1,024\n",
      "             ReLU-29             [1, 512, 2, 2]               0\n",
      "           UNetUP-30            [1, 1024, 2, 2]               0\n",
      "  ConvTranspose2d-31             [1, 512, 4, 4]       8,389,120\n",
      "      BatchNorm2d-32             [1, 512, 4, 4]           1,024\n",
      "             ReLU-33             [1, 512, 4, 4]               0\n",
      "           UNetUP-34            [1, 1024, 4, 4]               0\n",
      "  ConvTranspose2d-35             [1, 512, 8, 8]       8,389,120\n",
      "      BatchNorm2d-36             [1, 512, 8, 8]           1,024\n",
      "             ReLU-37             [1, 512, 8, 8]               0\n",
      "           UNetUP-38            [1, 1024, 8, 8]               0\n",
      "  ConvTranspose2d-39           [1, 256, 16, 16]       4,194,560\n",
      "      BatchNorm2d-40           [1, 256, 16, 16]             512\n",
      "             ReLU-41           [1, 256, 16, 16]               0\n",
      "           UNetUP-42           [1, 512, 16, 16]               0\n",
      "  ConvTranspose2d-43           [1, 128, 32, 32]       1,048,704\n",
      "      BatchNorm2d-44           [1, 128, 32, 32]             256\n",
      "             ReLU-45           [1, 128, 32, 32]               0\n",
      "           UNetUP-46           [1, 256, 32, 32]               0\n",
      "  ConvTranspose2d-47            [1, 64, 64, 64]         262,208\n",
      "      BatchNorm2d-48            [1, 64, 64, 64]             128\n",
      "             ReLU-49            [1, 64, 64, 64]               0\n",
      "           UNetUP-50           [1, 128, 64, 64]               0\n",
      "  ConvTranspose2d-51           [1, 4, 128, 128]           8,196\n",
      "           UNetUP-52           [1, 4, 128, 128]               0\n",
      "    LinearFeature-53           [1, 4, 128, 128]               8\n",
      "       ScaledTanh-54           [1, 4, 128, 128]               0\n",
      "================================================================\n",
      "Total params: 41,833,484\n",
      "Trainable params: 41,833,484\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 34.46\n",
      "Params size (MB): 159.58\n",
      "Estimated Total Size (MB): 194.11\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "STanh = cnn.ScaledTanh(30., 2./30.)\n",
    "LF = cnn.LinearFeature(4,4, bias=True)\n",
    "shape = (5,128,128)\n",
    "PIXGAN = gan.PIXGAN_WGP(\"sehgal_pixgan_251020\", shape, nconv_fcgen=64,\n",
    "                          nconv_fcdis=64, cuda=False, ngpu=4, nconv_layer_gen=4, nconv_layer_disc=5, kernal_size=4, stride=2,\n",
    "                          padding=1, output_padding=0, gen_act=[LF,STanh], nin_channel=1, nout_channel=4, nthresh_layer_gen=3, nthresh_layer_disc=0, dropout_rate=0)\n",
    "\n",
    "print(\"====pixgan======\")\n",
    "torchsummary.summary(PIXGAN.discriminator, shape, batch_size=1, device=\"cpu\")\n",
    "torchsummary.summary(PIXGAN.generator, (1,)+shape[-2:], batch_size=1, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====dc_wgp======\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 64, 64]           9,664\n",
      "         LeakyReLU-2           [-1, 64, 64, 64]               0\n",
      "            Conv2d-3          [-1, 128, 32, 32]         204,928\n",
      "         LeakyReLU-4          [-1, 128, 32, 32]               0\n",
      "            Conv2d-5          [-1, 256, 16, 16]         819,456\n",
      "         LeakyReLU-6          [-1, 256, 16, 16]               0\n",
      "            Conv2d-7            [-1, 512, 8, 8]       3,277,312\n",
      "         LeakyReLU-8            [-1, 512, 8, 8]               0\n",
      "            Conv2d-9            [-1, 512, 8, 8]       6,554,112\n",
      "      BatchNorm2d-10            [-1, 512, 8, 8]           1,024\n",
      "        LeakyReLU-11            [-1, 512, 8, 8]               0\n",
      "           Conv2d-12            [-1, 512, 8, 8]       6,554,112\n",
      "      BatchNorm2d-13            [-1, 512, 8, 8]           1,024\n",
      "        LeakyReLU-14            [-1, 512, 8, 8]               0\n",
      "           Conv2d-15              [-1, 1, 8, 8]          12,801\n",
      "          Sigmoid-16              [-1, 1, 8, 8]               0\n",
      "================================================================\n",
      "Total params: 17,434,433\n",
      "Trainable params: 17,434,433\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.38\n",
      "Forward/backward pass size (MB): 9.00\n",
      "Params size (MB): 66.51\n",
      "Estimated Total Size (MB): 75.88\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 64, 64]           4,864\n",
      "         LeakyReLU-2           [-1, 64, 64, 64]               0\n",
      "          UNetDown-3           [-1, 64, 64, 64]               0\n",
      "            Conv2d-4          [-1, 128, 32, 32]         204,928\n",
      "       BatchNorm2d-5          [-1, 128, 32, 32]             256\n",
      "         LeakyReLU-6          [-1, 128, 32, 32]               0\n",
      "          UNetDown-7          [-1, 128, 32, 32]               0\n",
      "            Conv2d-8          [-1, 256, 16, 16]         819,456\n",
      "       BatchNorm2d-9          [-1, 256, 16, 16]             512\n",
      "        LeakyReLU-10          [-1, 256, 16, 16]               0\n",
      "         UNetDown-11          [-1, 256, 16, 16]               0\n",
      "           Conv2d-12            [-1, 256, 8, 8]       1,638,656\n",
      "      BatchNorm2d-13            [-1, 256, 8, 8]             512\n",
      "        LeakyReLU-14            [-1, 256, 8, 8]               0\n",
      "         UNetDown-15            [-1, 256, 8, 8]               0\n",
      "           Conv2d-16            [-1, 256, 4, 4]       1,638,656\n",
      "      BatchNorm2d-17            [-1, 256, 4, 4]             512\n",
      "        LeakyReLU-18            [-1, 256, 4, 4]               0\n",
      "         UNetDown-19            [-1, 256, 4, 4]               0\n",
      "           Conv2d-20            [-1, 256, 2, 2]       1,638,656\n",
      "      BatchNorm2d-21            [-1, 256, 2, 2]             512\n",
      "        LeakyReLU-22            [-1, 256, 2, 2]               0\n",
      "         UNetDown-23            [-1, 256, 2, 2]               0\n",
      "           Conv2d-24            [-1, 256, 1, 1]       1,638,656\n",
      "             ReLU-25            [-1, 256, 1, 1]               0\n",
      "         UNetDown-26            [-1, 256, 1, 1]               0\n",
      "  ConvTranspose2d-27            [-1, 256, 2, 2]       1,638,656\n",
      "      BatchNorm2d-28            [-1, 256, 2, 2]             512\n",
      "             ReLU-29            [-1, 256, 2, 2]               0\n",
      "           UNetUP-30            [-1, 512, 2, 2]               0\n",
      "  ConvTranspose2d-31            [-1, 256, 4, 4]       3,277,056\n",
      "      BatchNorm2d-32            [-1, 256, 4, 4]             512\n",
      "          Dropout-33            [-1, 256, 4, 4]               0\n",
      "             ReLU-34            [-1, 256, 4, 4]               0\n",
      "           UNetUP-35            [-1, 512, 4, 4]               0\n",
      "  ConvTranspose2d-36            [-1, 256, 8, 8]       3,277,056\n",
      "      BatchNorm2d-37            [-1, 256, 8, 8]             512\n",
      "          Dropout-38            [-1, 256, 8, 8]               0\n",
      "             ReLU-39            [-1, 256, 8, 8]               0\n",
      "           UNetUP-40            [-1, 512, 8, 8]               0\n",
      "  ConvTranspose2d-41          [-1, 256, 16, 16]       3,277,056\n",
      "      BatchNorm2d-42          [-1, 256, 16, 16]             512\n",
      "          Dropout-43          [-1, 256, 16, 16]               0\n",
      "             ReLU-44          [-1, 256, 16, 16]               0\n",
      "           UNetUP-45          [-1, 512, 16, 16]               0\n",
      "  ConvTranspose2d-46          [-1, 128, 32, 32]       1,638,528\n",
      "      BatchNorm2d-47          [-1, 128, 32, 32]             256\n",
      "             ReLU-48          [-1, 128, 32, 32]               0\n",
      "           UNetUP-49          [-1, 256, 32, 32]               0\n",
      "  ConvTranspose2d-50           [-1, 64, 64, 64]         409,664\n",
      "      BatchNorm2d-51           [-1, 64, 64, 64]             128\n",
      "             ReLU-52           [-1, 64, 64, 64]               0\n",
      "           UNetUP-53          [-1, 128, 64, 64]               0\n",
      "  ConvTranspose2d-54          [-1, 3, 128, 128]           9,603\n",
      "           UNetUP-55          [-1, 3, 128, 128]               0\n",
      "             Tanh-56          [-1, 3, 128, 128]               0\n",
      "================================================================\n",
      "Total params: 21,116,227\n",
      "Trainable params: 21,116,227\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 32.76\n",
      "Params size (MB): 80.55\n",
      "Estimated Total Size (MB): 113.50\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "shape = (6,128,128)\n",
    "genin_shape = (3,128,128)\n",
    "discin_shape = (6,128,128)\n",
    "unet_gen = model.UNET_Generator(\n",
    "    genin_shape,\n",
    "    nconv_layer=3,\n",
    "    nconv_fc=64,\n",
    "    ngpu=1,\n",
    "    kernal_size=5,\n",
    "    stride=2,\n",
    "    padding=2,\n",
    "    output_padding=1,\n",
    "    normalize=True,\n",
    "    activation=[torch.nn.Tanh()],\n",
    "    nin_channel = 3,\n",
    "    nout_channel = 3,\n",
    "    nthresh_layer = 4\n",
    ").to(device=\"cpu\")\n",
    "\n",
    "unet_disc = model.UNET_Discriminator(\n",
    "    shape=discin_shape,\n",
    "    nconv_layer=4,\n",
    "    nconv_fc=64,\n",
    "    ngpu=1,\n",
    "    kernal_size=5,\n",
    "    stride=2,\n",
    "    padding=2,\n",
    "    normalize=False,\n",
    "    nthresh_layer=2\n",
    ").to(device=\"cpu\")\n",
    "\n",
    "\n",
    "print(\"====dc_wgp======\")\n",
    "torchsummary.summary(unet_disc, discin_shape, device=\"cpu\")\n",
    "torchsummary.summary(unet_gen, genin_shape, device=\"cpu\")\n",
    "#print(type(unet_disc.model))\n",
    "#torchsummary.summary(DCGAN_WGP.generator, (latent_dim,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 256 0 1\n",
      "256 128 1 2\n",
      "128 64 2 3\n",
      "64 32 3 4\n",
      "torch.Size([2, 256, 56, 56]) torch.Size([2, 256, 56, 56])\n",
      "torch.Size([2, 128, 112, 112]) torch.Size([2, 128, 112, 112])\n",
      "torch.Size([2, 64, 224, 224]) torch.Size([2, 64, 224, 224])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "       BatchNorm2d-2         [-1, 64, 224, 224]             128\n",
      "              ReLU-3         [-1, 64, 224, 224]               0\n",
      "            Conv2d-4         [-1, 64, 224, 224]          36,928\n",
      "            Conv2d-5         [-1, 64, 224, 224]           1,792\n",
      "      ResUNetBlock-6         [-1, 64, 224, 224]               0\n",
      "       BatchNorm2d-7         [-1, 64, 224, 224]             128\n",
      "              ReLU-8         [-1, 64, 224, 224]               0\n",
      "            Conv2d-9        [-1, 128, 112, 112]          73,856\n",
      "      BatchNorm2d-10        [-1, 128, 112, 112]             256\n",
      "             ReLU-11        [-1, 128, 112, 112]               0\n",
      "           Conv2d-12        [-1, 128, 112, 112]         147,584\n",
      "           Conv2d-13        [-1, 128, 112, 112]          73,856\n",
      "      BatchNorm2d-14        [-1, 128, 112, 112]             256\n",
      "     ResUNetBlock-15        [-1, 128, 112, 112]               0\n",
      "      BatchNorm2d-16        [-1, 128, 112, 112]             256\n",
      "             ReLU-17        [-1, 128, 112, 112]               0\n",
      "           Conv2d-18          [-1, 256, 56, 56]         295,168\n",
      "      BatchNorm2d-19          [-1, 256, 56, 56]             512\n",
      "             ReLU-20          [-1, 256, 56, 56]               0\n",
      "           Conv2d-21          [-1, 256, 56, 56]         590,080\n",
      "           Conv2d-22          [-1, 256, 56, 56]         295,168\n",
      "      BatchNorm2d-23          [-1, 256, 56, 56]             512\n",
      "     ResUNetBlock-24          [-1, 256, 56, 56]               0\n",
      "      BatchNorm2d-25          [-1, 256, 56, 56]             512\n",
      "             ReLU-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27          [-1, 512, 28, 28]       1,180,160\n",
      "      BatchNorm2d-28          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-29          [-1, 512, 28, 28]               0\n",
      "           Conv2d-30          [-1, 512, 28, 28]       2,359,808\n",
      "           Conv2d-31          [-1, 512, 28, 28]       1,180,160\n",
      "      BatchNorm2d-32          [-1, 512, 28, 28]           1,024\n",
      "     ResUNetBlock-33          [-1, 512, 28, 28]               0\n",
      "  ConvTranspose2d-34          [-1, 256, 56, 56]       1,179,904\n",
      "ResUNetUPInterface-35          [-1, 512, 56, 56]               0\n",
      "      BatchNorm2d-36          [-1, 512, 56, 56]           1,024\n",
      "             ReLU-37          [-1, 512, 56, 56]               0\n",
      "           Conv2d-38          [-1, 256, 56, 56]       1,179,904\n",
      "      BatchNorm2d-39          [-1, 256, 56, 56]             512\n",
      "             ReLU-40          [-1, 256, 56, 56]               0\n",
      "           Conv2d-41          [-1, 256, 56, 56]         590,080\n",
      "           Conv2d-42          [-1, 256, 56, 56]       1,179,904\n",
      "      BatchNorm2d-43          [-1, 256, 56, 56]             512\n",
      "     ResUNetBlock-44          [-1, 256, 56, 56]               0\n",
      "  ConvTranspose2d-45        [-1, 128, 112, 112]         295,040\n",
      "ResUNetUPInterface-46        [-1, 256, 112, 112]               0\n",
      "      BatchNorm2d-47        [-1, 256, 112, 112]             512\n",
      "             ReLU-48        [-1, 256, 112, 112]               0\n",
      "           Conv2d-49        [-1, 128, 112, 112]         295,040\n",
      "      BatchNorm2d-50        [-1, 128, 112, 112]             256\n",
      "             ReLU-51        [-1, 128, 112, 112]               0\n",
      "           Conv2d-52        [-1, 128, 112, 112]         147,584\n",
      "           Conv2d-53        [-1, 128, 112, 112]         295,040\n",
      "      BatchNorm2d-54        [-1, 128, 112, 112]             256\n",
      "     ResUNetBlock-55        [-1, 128, 112, 112]               0\n",
      "  ConvTranspose2d-56         [-1, 64, 224, 224]          73,792\n",
      "ResUNetUPInterface-57        [-1, 128, 224, 224]               0\n",
      "      BatchNorm2d-58        [-1, 128, 224, 224]             256\n",
      "             ReLU-59        [-1, 128, 224, 224]               0\n",
      "           Conv2d-60         [-1, 64, 224, 224]          73,792\n",
      "      BatchNorm2d-61         [-1, 64, 224, 224]             128\n",
      "             ReLU-62         [-1, 64, 224, 224]               0\n",
      "           Conv2d-63         [-1, 64, 224, 224]          36,928\n",
      "           Conv2d-64         [-1, 64, 224, 224]          73,792\n",
      "      BatchNorm2d-65         [-1, 64, 224, 224]             128\n",
      "     ResUNetBlock-66         [-1, 64, 224, 224]               0\n",
      "           Conv2d-67          [-1, 3, 224, 224]             195\n",
      "             Tanh-68          [-1, 3, 224, 224]               0\n",
      "================================================================\n",
      "Total params: 11,665,539\n",
      "Trainable params: 11,665,539\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 985.36\n",
      "Params size (MB): 44.50\n",
      "Estimated Total Size (MB): 1030.43\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "shape = (3,128,128)\n",
    "genin_shape = (3,224,224)\n",
    "discin_shape = (6,128,128)\n",
    "unet_gen = model.ResUNET_Generator(\n",
    "    genin_shape,\n",
    "    nconv_layer=3,\n",
    "    nconv_fc=64,\n",
    "    ngpu=1,\n",
    "    activation=[torch.nn.Tanh()],\n",
    "    nin_channel = 3,\n",
    "    nout_channel = 3,\n",
    "    nthresh_layer = 1,\n",
    ").to(device=\"cpu\")\n",
    "\n",
    "torchsummary.summary(unet_gen, genin_shape, device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResUNetUPInterface(\n",
       "  (model): Sequential(\n",
       "    (0): ConvTranspose2d(512, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet_gen.model_dict['up0_int']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResUNetBlock(\n",
       "  (model): Sequential(\n",
       "    (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (imodel): Sequential(\n",
       "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet_gen.model_dict['up0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm2d-1          [-1, 3, 128, 128]               6\n",
      "              ReLU-2          [-1, 3, 128, 128]               0\n",
      "            Conv2d-3         [-1, 64, 128, 128]           1,792\n",
      "       BatchNorm2d-4         [-1, 64, 128, 128]             128\n",
      "              ReLU-5         [-1, 64, 128, 128]               0\n",
      "            Conv2d-6         [-1, 64, 128, 128]          36,928\n",
      "            Conv2d-7         [-1, 64, 128, 128]           1,792\n",
      "================================================================\n",
      "Total params: 40,646\n",
      "Trainable params: 40,646\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 40.75\n",
      "Params size (MB): 0.16\n",
      "Estimated Total Size (MB): 41.09\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "shape = (3,128,128)\n",
    "genin_shape = (3,128,128)\n",
    "unet_gen = model.ResUNetDown(\n",
    "    in_filters=3, \n",
    "    out_filters=64, \n",
    "    outermost=True, \n",
    "    kernal_size=3,\n",
    "    padding=1, ngpu=0, use_leaky=False\n",
    ").to(device=\"cpu\")\n",
    "\n",
    "torchsummary.summary(unet_gen, genin_shape, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
